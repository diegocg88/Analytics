{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Executive Summary </strong> <br>\n",
    "\n",
    "<strong> Created by. </strong> <br> Diego Cruz-Galarza\n",
    "\n",
    "<strong> Case. Apprentice Chef, Inc.</strong> <br>\n",
    "\n",
    "<strong> Context. </strong> Apprentice Chef, Inc. is a start-up which offers meals cooked  at home under 30 minutes. The target audience are busy professional with little to no skills in the kitchen. The company preferred channels are an on-line platform and a mobile application. \n",
    "\n",
    "General Product Specifications.\n",
    "    \n",
    "   - 30 min to finish cooking.\n",
    "   - Disposable cookware.\n",
    "   - Delicious and healthy eating.\n",
    "<br>\n",
    "<br>  \n",
    "<br> \n",
    "\n",
    "\n",
    "<strong> Revenue Goal. </strong> Analysis on how the revenue is influenced by customers within the first year of using the service.\n",
    "\n",
    "<strong> Revenue Insights. </strong> \n",
    "\n",
    " - Total meals ordered and largest order size contribute much to the revenue since it is Quantity x Price.\n",
    " \n",
    " - Unique meals purchased have a negative relationship with revenue. This reflects how customers tend to opt for the same meal. It would be interesting to compare the kind of meals and analyze the possibility of increasing the menu.\n",
    "  \n",
    " - There is an interesting dip in Revenue when it reaches approx 2k.\n",
    " \n",
    " \n",
    "<strong> Revenue Results. </strong>  Our best model was a OLS with r squared 0.732. It predicts how the revenue is impacted by the variables collected.\n",
    "<br>\n",
    "![<caption>](results_rev.png)\n",
    "<kbd> Results from analyzing the variable revenue.</kbd>\n",
    "<br>\n",
    "\n",
    "<strong> Cross Sell Promotion Goal. </strong> Analysis of success on the new subscription where customers receive half bottle of wine every Wednesday from a local California vineyard.\n",
    "\n",
    "<strong> Cross Sell Promotion Insights. </strong> \n",
    "\n",
    " - Junk emails created a bad correlation with cross sell promotions. It would be nice to implement an email marketing campaign to these customers and see the results. If we see potential in this group we might consider running another campaign in a different platform.\n",
    "\n",
    " - If communications about the subscription are through email, professionals and personal emails are the ones that are going to open and click on it \n",
    "\n",
    "<strong> Cross Sell Promotion Results. </strong> Our best performing model was Tuned Tree with an AUC score of 0.855. Predicting whether or not a customer will subscribe to the new service.\n",
    "    \n",
    "![<caption>](results_css.png) \n",
    "<kbd> Results from analyzing cross sell promotion.</kbd>\n",
    "<br>\n",
    "<br>  \n",
    "<br>\n",
    "\n",
    "<strong> Code. </strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd                                       # data science essentials\n",
    "import matplotlib.pyplot as plt                           # essential graphical output\n",
    "import seaborn as sns                                     # enhanced graphical output\n",
    "import numpy as np                                        # mathematical functions, random number generators, and more\n",
    "import random as rand                                     # random number gen\n",
    "import statsmodels.formula.api as smf                     # predictive modeling with nice outputs\n",
    "from sklearn.model_selection import train_test_split      # train test split\n",
    "from sklearn.neighbors import KNeighborsRegressor         # KNN for Regression\n",
    "from sklearn.preprocessing import StandardScaler          # standard scaler\n",
    "from sklearn.linear_model import LinearRegression         # linear regression (scikit-learn)\n",
    "from sklearn.model_selection import cross_val_score       # cross-validation \n",
    "import sklearn                                            # machine learning library\n",
    "from sklearn.neighbors import KNeighborsRegressor         # KNN for Regression\n",
    "from sklearn.preprocessing import StandardScaler          # standard scaler\n",
    "from sklearn.model_selection import train_test_split      # train-test split\n",
    "from sklearn.linear_model import LogisticRegression       # logistic regression\n",
    "import statsmodels.formula.api as smf                     # logistic regression\n",
    "from sklearn.metrics import confusion_matrix              # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score                 # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier        # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor         # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler          # standard scaler\n",
    "from sklearn.tree import DecisionTreeClassifier           # classification trees\n",
    "from sklearn.tree import export_graphviz                  # exports graphics\n",
    "from six import StringIO                                  # saves objects in memory\n",
    "from IPython.display import Image                         # displays on frontend\n",
    "import pydotplus                                          # interprets dot objects\n",
    "from sklearn.model_selection import RandomizedSearchCV    # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                   # customizable scorer\n",
    "from sklearn.ensemble import RandomForestClassifier       # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier   # gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying pandas file name\n",
    "file = './datasets/Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "# loading data\n",
    "chef =pd.read_excel(io= file) \n",
    "\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "# displaying the head of the dataset\n",
    "chef.head(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Data Visualization. </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual EDA (Scatterplots) N1\n",
    "\n",
    "# setting figure size\n",
    "fig, ax = plt.subplots(figsize = (16, 14))\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.scatterplot(x = chef['AVG_PREP_VID_TIME'],\n",
    "                y = chef['REVENUE'],\n",
    "                color = 'g')\n",
    "\n",
    "\n",
    "# adding labels but not adding title\n",
    "plt.xlabel(xlabel = 'AVG_PREP_VID_TIME')\n",
    "plt.ylabel(ylabel = 'REVENUE')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.scatterplot(x = chef['MEDIAN_MEAL_RATING'],\n",
    "                y = chef['REVENUE'],\n",
    "                color = 'g')\n",
    "\n",
    "\n",
    "# adding labels but not adding title\n",
    "plt.xlabel(xlabel = 'MEDIAN_MEAL_RATING')\n",
    "plt.ylabel(ylabel = 'REVENUE')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.scatterplot(x = chef['TOTAL_MEALS_ORDERED'],\n",
    "                y = chef['REVENUE'],\n",
    "                color = 'orange')\n",
    "\n",
    "\n",
    "# adding labels but not adding title\n",
    "plt.xlabel(xlabel = 'TOTAL_MEALS_ORDERED')\n",
    "plt.ylabel(ylabel = 'REVENUE')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.scatterplot(x = chef['LARGEST_ORDER_SIZE'],\n",
    "                y = chef['REVENUE'],\n",
    "                color = 'r')\n",
    "\n",
    "\n",
    "# adding labels but not adding title\n",
    "plt.xlabel(xlabel = 'LARGEST_ORDER_SIZE')\n",
    "plt.ylabel(ylabel = 'REVENUE')\n",
    "\n",
    "\n",
    "# cleaning up the layout, saving the figures, and displaying the results\n",
    "plt.tight_layout()\n",
    "plt.savefig('./analysis_images/Chef pro Scatterplots 1 of 6.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual EDA (Scatterplots) N2\n",
    "\n",
    "# setting figure size\n",
    "fig, ax = plt.subplots(figsize = (16, 14))\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.scatterplot(x = chef['MASTER_CLASSES_ATTENDED'],\n",
    "                y = chef['REVENUE'],\n",
    "                color = 'y')\n",
    "\n",
    "\n",
    "# adding labels but not adding title\n",
    "plt.xlabel(xlabel = 'MASTER_CLASSES_ATTENDED')\n",
    "plt.ylabel(ylabel = 'REVENUE')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.scatterplot(x = chef['TOTAL_PHOTOS_VIEWED'],\n",
    "                y = chef['REVENUE'],\n",
    "                color = 'orange')\n",
    "\n",
    "\n",
    "# adding labels but not adding title\n",
    "plt.xlabel(xlabel = 'TOTAL_PHOTOS_VIEWED')\n",
    "plt.ylabel(ylabel = 'REVENUE')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.scatterplot(x = chef['AVG_TIME_PER_SITE_VISIT'],\n",
    "                y = chef['REVENUE'],\n",
    "                color = 'g')\n",
    "\n",
    "\n",
    "# adding labels but not adding title\n",
    "plt.xlabel(xlabel = 'AVG_TIME_PER_SITE_VISIT')\n",
    "plt.ylabel(ylabel = 'REVENUE')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "\n",
    "# developing a scatterplot\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.scatterplot(x = chef['CONTACTS_W_CUSTOMER_SERVICE'],\n",
    "                y = chef['REVENUE'],\n",
    "                color = 'r')\n",
    "\n",
    "\n",
    "# adding labels but not adding title\n",
    "plt.xlabel(xlabel = 'CONTACTS_W_CUSTOMER_SERVICE')\n",
    "plt.ylabel(ylabel = 'REVENUE')\n",
    "\n",
    "# cleaning up the layout, saving the figures, and displaying the results\n",
    "plt.tight_layout()\n",
    "plt.savefig('./analysis_images/Chef pro Scatterplots 2 of 6.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Data Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log to continuos variables\n",
    "\n",
    "# log transforming AVG_PREP_VID_TIME and saving it to the dataset\n",
    "chef['log_AVG_PREP_VID_TIME'] = np.log10(chef['AVG_PREP_VID_TIME'])\n",
    "\n",
    "# log transforming AVG_TIME_PER_SITE_VISIT and saving it to the dataset\n",
    "chef['log_AVG_TIME_PER_SITE_VISIT'] = np.log10(chef['AVG_TIME_PER_SITE_VISIT'])\n",
    "\n",
    "# log transforming AVG_PREP_VID_TIME and saving it to the dataset\n",
    "chef['log_AVG_PREP_VID_TIME'] = np.log10(chef['AVG_PREP_VID_TIME'])\n",
    "\n",
    "# log transforming TOTAL_MEALS_ORDERED and saving it to the dataset\n",
    "chef['log_TOTAL_MEALS_ORDERED'] = np.log10(chef['TOTAL_MEALS_ORDERED'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing some variables into binary\n",
    "\n",
    "# dummy variable for having a basement, creating new column fill of 0\n",
    "chef['has_TOTAL_PHOTOS_VIEWED']         = 0\n",
    "chef['has_MASTER_CLASSES_ATTENDED']     = 0\n",
    "chef['has_LATE_DELIVERIES']             = 0\n",
    "chef['has_EARLY_DELIVERIES']            = 0\n",
    "chef['has_CANCELLATIONS_AFTER_NOON']    = 0\n",
    "chef['has_CANCELLATIONS_BEFORE_NOON']   = 0\n",
    "chef['has_WEEKLY_PLAN']                 = 0\n",
    "chef['has_UNIQUE_MEALS_PURCH']          = 0\n",
    "chef['has_AVG_CLICKS_PER_VISIT']        = 0\n",
    "\n",
    "\n",
    "# iterating over each original column to\n",
    "# change values in the new feature columns\n",
    "for index, value in chef.iterrows():\n",
    "    \n",
    "    # TOTAL_PHOTOS_VIEWED\n",
    "    if chef.loc[index, 'TOTAL_PHOTOS_VIEWED'] > 0:\n",
    "        chef.loc[index, 'has_TOTAL_PHOTOS_VIEWED'] = 1\n",
    "\n",
    "\n",
    "    # MASTER_CLASSES_ATTENDED\n",
    "    if chef.loc[index, 'MASTER_CLASSES_ATTENDED'] > 0:\n",
    "        chef.loc[index, 'has_MASTER_CLASSES_ATTENDED'] = 1\n",
    "        \n",
    "        \n",
    "    # LATE_DELIVERIES\n",
    "    if chef.loc[index, 'LATE_DELIVERIES'] < 0.1:\n",
    "        chef.loc[index, 'has_LATE_DELIVERIES'] = 1\n",
    "        \n",
    "        \n",
    "    # EARLY_DELIVERIES\n",
    "    if chef.loc[index, 'EARLY_DELIVERIES'] < 0.1:\n",
    "        chef.loc[index, 'has_EARLY_DELIVERIES'] = 1\n",
    "        \n",
    "        \n",
    "    # CANCELLATIONS_AFTER_NOON\n",
    "    if chef.loc[index, 'CANCELLATIONS_AFTER_NOON'] > 0:\n",
    "        chef.loc[index, 'has_CANCELLATIONS_AFTER_NOON'] = 1\n",
    "        \n",
    "    # CANCELLATIONS_BEFORE_NOON\n",
    "    if chef.loc[index, 'CANCELLATIONS_BEFORE_NOON'] > 0:\n",
    "        chef.loc[index, 'has_CANCELLATIONS_BEFORE_NOON'] = 1\n",
    "        \n",
    "    # WEEKLY_PLAN\n",
    "    if chef.loc[index, 'WEEKLY_PLAN'] > 0:\n",
    "        chef.loc[index, 'has_WEEKLY_PLAN'] = 1\n",
    "        \n",
    "    # UNIQUE_MEALS_PURCH\n",
    "    if chef.loc[index, 'UNIQUE_MEALS_PURCH'] < 2:\n",
    "        chef.loc[index, 'has_UNIQUE_MEALS_PURCH'] = 1\n",
    "        \n",
    "    # AVG_CLICKS_PER_VISIT\n",
    "    if chef.loc[index, 'AVG_CLICKS_PER_VISIT'] < 11:\n",
    "        chef.loc[index, 'has_AVG_CLICKS_PER_VISIT'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new variables frequency of meals purchase\n",
    "\n",
    "chef['has_freq'] = chef['TOTAL_MEALS_ORDERED'] / chef['UNIQUE_MEALS_PURCH']\n",
    "\n",
    "chef['has_contact'] = chef['CONTACTS_W_CUSTOMER_SERVICE'] / chef['TOTAL_MEALS_ORDERED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new variable has_revenue. looking for a correlation between revenue and cross_sell_success\n",
    "chef['has_revenue']   = 0\n",
    "\n",
    "for index, value in chef.iterrows():\n",
    "    \n",
    "    \n",
    "    if chef.loc[index, 'REVENUE'] >= 0 and chef.loc[index, 'REVENUE'] <= 1000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==0 :\n",
    "        chef.loc[index, 'has_revenue'] = 0\n",
    "        \n",
    "    if chef.loc[index, 'REVENUE'] >= 1001 and chef.loc[index, 'REVENUE'] <= 2000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==0 :\n",
    "        chef.loc[index, 'has_revenue'] = 1\n",
    "\n",
    "    if chef.loc[index, 'REVENUE'] >= 2001 and chef.loc[index, 'REVENUE'] <= 3000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==0 :\n",
    "        chef.loc[index, 'has_revenue'] = 2\n",
    "    \n",
    "    if chef.loc[index, 'REVENUE'] >= 3001 and chef.loc[index, 'REVENUE'] <= 4000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==0 :\n",
    "        chef.loc[index, 'has_revenue'] = 3\n",
    "        \n",
    "    if chef.loc[index, 'REVENUE'] >= 4001 and chef.loc[index, 'REVENUE'] <= 5000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==0 :\n",
    "        chef.loc[index, 'has_revenue'] = 4\n",
    "        \n",
    "    if chef.loc[index, 'REVENUE'] >= 5001 and chef.loc[index, 'CROSS_SELL_SUCCESS']==0 :\n",
    "        chef.loc[index, 'has_revenue'] = 5\n",
    "        \n",
    "    if chef.loc[index, 'REVENUE'] >= 0 and chef.loc[index, 'REVENUE'] <= 1000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==1 :\n",
    "        chef.loc[index, 'has_revenue'] = 1\n",
    "    \n",
    "    if chef.loc[index, 'REVENUE'] >= 1001 and chef.loc[index, 'REVENUE'] <= 2000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==1 :\n",
    "        chef.loc[index, 'has_revenue'] = 2\n",
    "    \n",
    "    if chef.loc[index, 'REVENUE'] >= 2001 and chef.loc[index, 'REVENUE'] <= 3000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==1 :\n",
    "        chef.loc[index, 'has_revenue'] = 3\n",
    "        \n",
    "    if chef.loc[index, 'REVENUE'] >= 3001 and chef.loc[index, 'REVENUE'] <= 4000 and chef.loc[index, 'CROSS_SELL_SUCCESS']==1 :\n",
    "        chef.loc[index, 'has_revenue'] = 4\n",
    "    \n",
    "    if chef.loc[index, 'REVENUE'] >= 4001 and chef.loc[index, 'CROSS_SELL_SUCCESS']==1 :\n",
    "        chef.loc[index, 'has_revenue'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all variables are above 200 observations so they can be considered for the analysis\n",
    "\n",
    "chef_d = chef.copy()\n",
    "\n",
    "# STEP 1: splitting emails\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in chef.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = chef_d.loc[index, 'EMAIL'].split(sep = '@')\n",
    "\n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "# converting placeholder_lst into a DataFrame\n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "\n",
    "# STEP 2: concatenating with original DataFrame\n",
    "\n",
    "# renaming column to concatenate\n",
    "email_df.columns = ['name' , 'EMAIL_DOMAIN'] \n",
    "\n",
    "# concatenating personal_email_domain with chef DataFrame\n",
    "chef = pd.concat([chef, email_df.loc[:, 'EMAIL_DOMAIN']], \n",
    "                   axis = 1)\n",
    "\n",
    "# printing value counts of personal_email_domain\n",
    "chef.loc[: ,'EMAIL_DOMAIN'].value_counts()\n",
    "\n",
    "# STEP 3: split emails into personal and professional\n",
    "# email domain types\n",
    "professional_email_domains = ['@mmm.com',         '@amex.com',\n",
    "                              '@apple.com',       '@boeing.com',\n",
    "                              '@caterpillar.com', '@chevron.com',\n",
    "                              '@cisco.com',       '@cocacola.com',\n",
    "                              '@disney.com',      '@dupont.com',\n",
    "                              '@exxon.com',       '@ge.org',\n",
    "                              '@goldmansacs.com', '@homedepot.com',\n",
    "                              '@ibm.com',         '@intel.com',\n",
    "                              '@jnj.com',         '@jpmorgan.com',\n",
    "                              '@mcdonalds.com',   '@merck.com',\n",
    "                              '@microsoft.com',   '@nike.com',\n",
    "                              '@pfizer.com',      '@pg.com',\n",
    "                              '@travelers.com',   '@unitedtech.com',\n",
    "                              '@unitedhealth.com','@verizon.com',\n",
    "                              '@visa.com',        '@walmart.com']\n",
    "personal_email_domains     = ['@gmail.com',       '@yahoo.com',    \n",
    "                              '@protonmail.com',]\n",
    "junk_email_domains         = ['@me.com',          '@aol.com',\n",
    "                              '@hotmail.com',     '@live.com',        \n",
    "                              '@msn.com',         '@passport.com']\n",
    "\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []  \n",
    "\n",
    "\n",
    "# looping to group observations by domain type\n",
    "for domain in chef['EMAIL_DOMAIN']:\n",
    "        if \"@\" + domain in professional_email_domains:\n",
    "            placeholder_lst.append('professional')\n",
    "            \n",
    "        elif \"@\" + domain in personal_email_domains:\n",
    "            placeholder_lst.append('personal')\n",
    "            \n",
    "        elif \"@\" + domain in junk_email_domains:\n",
    "            placeholder_lst.append('junk')\n",
    "            \n",
    "        else:\n",
    "            print('Unknown')\n",
    "\n",
    "\n",
    "# concatenating with original DataFrame\n",
    "chef['email_domain_group'] = pd.Series(placeholder_lst)\n",
    "\n",
    "# checking results and sample size\n",
    "print(chef['email_domain_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing some variables into binary\n",
    "\n",
    "# dummy variable for having a basement, creating new column fill of 0\n",
    "chef['has_TOTAL_PHOTOS_VIEWED']         = 0\n",
    "chef['has_MASTER_CLASSES_ATTENDED']     = 0\n",
    "chef['has_EARLY_DELIVERIES']            = 0\n",
    "chef['has_WEEKLY_PLAN']                 = 0\n",
    "chef['has_AVG_CLICKS_PER_VISIT']        = 0\n",
    "chef['has_professional']                = 0\n",
    "chef['has_personal']                    = 0\n",
    "chef['has_PRODUCT_CATEGORIES_VIEWED']   = 0\n",
    "chef['has_MOBILE_LOGINS']               = 0\n",
    "\n",
    "\n",
    "# iterating over each original column to\n",
    "# change values in the new feature columns\n",
    "for index, value in chef.iterrows():\n",
    "    \n",
    "    # TOTAL_PHOTOS_VIEWED\n",
    "    if chef.loc[index, 'TOTAL_PHOTOS_VIEWED'] > 0:\n",
    "        chef.loc[index, 'has_TOTAL_PHOTOS_VIEWED'] = 1\n",
    "\n",
    "    # MASTER_CLASSES_ATTENDED\n",
    "    if chef.loc[index, 'MASTER_CLASSES_ATTENDED'] > 0:\n",
    "        chef.loc[index, 'has_MASTER_CLASSES_ATTENDED'] = 1\n",
    "        \n",
    "    # EARLY_DELIVERIES\n",
    "    if chef.loc[index, 'EARLY_DELIVERIES'] < 0.1:\n",
    "        chef.loc[index, 'has_EARLY_DELIVERIES'] = 1\n",
    "\n",
    "    # WEEKLY_PLAN\n",
    "    if chef.loc[index, 'WEEKLY_PLAN'] > 10:\n",
    "        chef.loc[index, 'has_WEEKLY_PLAN'] = 1\n",
    "        \n",
    "    # AVG_CLICKS_PER_VISIT\n",
    "    if chef.loc[index, 'AVG_CLICKS_PER_VISIT'] < 7:\n",
    "        chef.loc[index, 'has_AVG_CLICKS_PER_VISIT'] = 1\n",
    "           \n",
    "    # Professional Emails\n",
    "    if chef.loc[index, 'email_domain_group'] == \"professional\":\n",
    "        chef.loc[index, 'has_professional'] = 1\n",
    "        \n",
    "    # Personal Emails\n",
    "    if chef.loc[index, 'email_domain_group'] == \"personal\":\n",
    "        chef.loc[index, 'has_personal'] = 1        \n",
    "                \n",
    "    # PRODUCT_CATEGORIES_VIEWED. improved correlation\n",
    "    if chef.loc[index, 'PRODUCT_CATEGORIES_VIEWED'] > 9:\n",
    "        chef.loc[index, 'has_PRODUCT_CATEGORIES_VIEWED'] = 1  \n",
    "        \n",
    "    # MOBILE_LOGINS\n",
    "    if chef.loc[index, 'MOBILE_LOGINS'] > 0.1:\n",
    "        chef.loc[index, 'has_MOBILE_LOGINS'] = 1 \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Variables. </strong>\n",
    "\n",
    "<strong> Early Deliveries.</strong> Considered that 0 is success and all the other obs 0. Customers want the order when is due.\n",
    "    \n",
    "<strong> Late Deliveries.</strong> Considered that 0 is success and all the other obs 0. Customers want the order when is due.\n",
    "    \n",
    "<strong> Unique Meal Purchase.</strong> Considered 1 is success and all the other obs 0. Customers that ordered just 1 time are not good for revenue. \n",
    "                      \n",
    "<strong> Avg Click Visit.</strong> . Considered that above 11 are no success = 0. Customers that are long time in the web page are usually confuse about what to order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Revenue Models. </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing a small correlation matrix for profesional emails\n",
    "chef_corr = chef.corr(method = 'pearson')    \n",
    "\n",
    "# filtering the results to only show correlations with Revenue\n",
    "chef_corr.loc[ : , 'REVENUE'].round(decimals = 2).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame for continuos variables\n",
    "chef_select = chef.loc[ : , ['REVENUE','AVG_PREP_VID_TIME','TOTAL_MEALS_ORDERED','has_freq',\n",
    "                             'TOTAL_PHOTOS_VIEWED','LARGEST_ORDER_SIZE','MASTER_CLASSES_ATTENDED']]\n",
    "\n",
    "chef_select.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing a small correlation matrix for profesional emails\n",
    "chef_select_corr = chef_select.corr(method = 'pearson')    \n",
    "\n",
    "# filtering the results to only show correlations with Revenue\n",
    "chef_select_corr.loc[ : , 'REVENUE'].round(decimals = 2).sort_values(ascending = False)\n",
    "\n",
    "# utilizing palplot to create a color map\n",
    "sns.palplot(sns.color_palette('viridis', 10))\n",
    "\n",
    "#plot size\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.set(font_scale =2)\n",
    "\n",
    "#creating heat map\n",
    "sns.heatmap(data = chef_select_corr,\n",
    "            cmap = 'viridis',\n",
    "            square = True,\n",
    "            annot = True,\n",
    "            linecolor = 'white',\n",
    "            linewidths = 0.5,\n",
    "            cbar = False)\n",
    "\n",
    "#editing the map\n",
    "bottom, top = plt.ylim()\n",
    "bottom += 0.5            \n",
    "top -= 0.5               \n",
    "plt.ylim(bottom, top)\n",
    "\n",
    "#saving the figure and displaying the plot\n",
    "plt.savefig('Chef Correlation Heatmap Continous Variables.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training and testing data sets for models\n",
    "\n",
    "# droping variables that the model dont need\n",
    "chef_data   = chef.drop(['NAME',\n",
    "                         'EMAIL',\n",
    "                         'FIRST_NAME',\n",
    "                         'FAMILY_NAME',\n",
    "                         'EMAIL_DOMAIN',\n",
    "                         'email_domain_group'\n",
    "                         ],\n",
    "                         axis = 1)\n",
    "\n",
    "\n",
    "# preparing response variables\n",
    "chef_target = chef.loc[ : , 'REVENUE']\n",
    "log_chef_target = chef.loc[ : , 'REVENUE']\n",
    "\n",
    "\n",
    "# preparing training and testing sets (all letters are lowercase)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)\n",
    "\n",
    "\n",
    "# checking the shapes of the datasets\n",
    "print(f\"\"\"\n",
    "Training Data\n",
    "-------------\n",
    "X-side: {x_train.shape}\n",
    "y-side: {y_train.shape}\n",
    "\n",
    "\n",
    "Testing Data\n",
    "------------\n",
    "X-side: {x_test.shape}\n",
    "y-side: {y_test.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# looping to make x-variables suitable for statsmodels\n",
    "for val in chef_data:\n",
    "    print(f\"'{val}',\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring set of x-variables\n",
    "x_variables = [ 'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "                                            'AVG_PREP_VID_TIME',\n",
    "                                            'LARGEST_ORDER_SIZE',\n",
    "                                            'MASTER_CLASSES_ATTENDED',\n",
    "                                            'MEDIAN_MEAL_RATING',\n",
    "                                            'TOTAL_PHOTOS_VIEWED',\n",
    "                                            'log_AVG_TIME_PER_SITE_VISIT',\n",
    "                                            'log_TOTAL_MEALS_ORDERED',\n",
    "                                            'has_UNIQUE_MEALS_PURCH',\n",
    "                                            'has_freq',\n",
    "                                            'has_contact']\n",
    "\n",
    "# looping to make x-variables suitable for statsmodels\n",
    "for val in x_variables:\n",
    "    print(f\"{val} +\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDINARY LEAST SQUARES REGRESSION\n",
    "\n",
    "# merging X_train and y_train so that they can be used in statsmodels\n",
    "chef_train = pd.concat([x_train, y_train], axis = 1)\n",
    "\n",
    "\n",
    "# Step 1: build a model\n",
    "lm_best = smf.ols(formula =  \"\"\"REVENUE ~   \n",
    "                                            CONTACTS_W_CUSTOMER_SERVICE +\n",
    "                                            AVG_PREP_VID_TIME +\n",
    "                                            LARGEST_ORDER_SIZE +\n",
    "                                            MASTER_CLASSES_ATTENDED +\n",
    "                                            MEDIAN_MEAL_RATING +\n",
    "                                            TOTAL_PHOTOS_VIEWED +\n",
    "                                            log_AVG_TIME_PER_SITE_VISIT +\n",
    "                                            log_TOTAL_MEALS_ORDERED +\n",
    "                                            has_UNIQUE_MEALS_PURCH +\n",
    "                                            has_freq +\n",
    "                                            has_contact\n",
    "                                                \"\"\",\n",
    "                                data = chef)\n",
    "\n",
    "\n",
    "# Step 2: fit the model based on the data\n",
    "results = lm_best.fit()\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: analyze the summary output\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Model in SCIKIT-LEARN \n",
    "\n",
    "# preparing x-variables from the OLS model\n",
    "chef_data = chef[x_variables]\n",
    "\n",
    "\n",
    "# preparing response variable\n",
    "chef_target = chef['REVENUE']\n",
    "\n",
    "\n",
    "###############################################\n",
    "## setting up more than one train-test split ##\n",
    "###############################################\n",
    "\n",
    "# FULL X-dataset (normal Y)\n",
    "x_train_FULL, x_test_FULL, y_train_FULL, y_test_FULL = train_test_split(\n",
    "            chef_data,     # x-variables\n",
    "            chef_target,   # y-variable\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)\n",
    "\n",
    "\n",
    "# OLS p-value x-dataset (normal Y)\n",
    "x_train_OLS, x_test_OLS, y_train_OLS, y_test_OLS = train_test_split(\n",
    "            chef_data,         # x-variables\n",
    "            chef_target,   # y-variable\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a Linear Regression model object\n",
    "lr = LinearRegression()\n",
    "\n",
    "\n",
    "# FITTING to the training data\n",
    "lr_fit = lr.fit(x_train_OLS, y_train_OLS)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "lr_pred = lr_fit.predict(x_test_OLS)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('OLS Training Score :', lr.score(x_train_OLS, y_train_OLS).round(4))  # using R-square\n",
    "print('OLS Testing Score  :',  lr.score(x_test_OLS, y_test_OLS).round(4)) # using R-square\n",
    "\n",
    "lr_train_score = lr.score(x_train_OLS, y_train_OLS).round(4)\n",
    "lr_test_score  = lr.score(x_test_OLS, y_test_OLS).round(4)\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('OLS Train-Test Gap :', abs(lr_train_score - lr_test_score).round(4))\n",
    "lr_test_gap = abs(lr_train_score - lr_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "lr_model_values = zip(chef[x_variables].columns,\n",
    "                      lr_fit.coef_.round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "lr_model_lst = [('intercept', lr_fit.intercept_.round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in lr_model_values:\n",
    "    lr_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in lr_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO REGRESSION MODEL\n",
    "\n",
    "# INSTANTIATING a model object\n",
    "lasso_model = sklearn.linear_model.Lasso(alpha = 1.0,\n",
    "                                         normalize = True) # default magitude\n",
    "\n",
    "\n",
    "# FITTING to the training data\n",
    "lasso_fit = lasso_model.fit(x_train_FULL, y_train_FULL)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "lasso_pred = lasso_fit.predict(x_test_FULL)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Lasso Training Score :', lasso_model.score(x_train_FULL, y_train_FULL).round(4))\n",
    "print('Lasso Testing Score  :', lasso_model.score(x_test_FULL, y_test_FULL).round(4))\n",
    "\n",
    "\n",
    "## the following code has been provided for you ##\n",
    "\n",
    "# saving scoring data for future use\n",
    "lasso_train_score = lasso_model.score(x_train_FULL, y_train_FULL).round(4) # using R-square\n",
    "lasso_test_score  = lasso_model.score(x_test_FULL, y_test_FULL).round(4)   # using R-square\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Lasso Train-Test Gap :', abs(lr_train_score - lr_test_score).round(4))\n",
    "lasso_test_gap = abs(lr_train_score - lr_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# zipping each feature name to its coefficient\n",
    "lasso_model_values = zip(chef.columns, lasso_fit.coef_.round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "lasso_model_lst = [('intercept', lasso_fit.intercept_.round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in lasso_model_values:\n",
    "    lasso_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in lasso_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code may have to be run more than once ##\n",
    "\n",
    "# dropping coefficients that are equal to zero\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for feature, coefficient in lasso_model_lst:\n",
    "        \n",
    "        if coefficient == 0:\n",
    "            lasso_model_lst.remove((feature, coefficient))\n",
    "\n",
    "            \n",
    "# checking the results\n",
    "for pair in lasso_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARD MODEL\n",
    "\n",
    "# INSTANTIATING a model object\n",
    "ard_model = sklearn.linear_model.ARDRegression()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "ard_fit = ard_model.fit(x_train_FULL, y_train_FULL)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "ard_pred = ard_fit.predict(x_test_FULL)\n",
    "\n",
    "\n",
    "print('Training Score:', ard_model.score(x_train_FULL, y_train_FULL).round(4))\n",
    "print('Testing Score :',  ard_model.score(x_test_FULL, y_test_FULL).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "ard_train_score = ard_model.score(x_train_FULL, y_train_FULL).round(4)\n",
    "ard_test_score  = ard_model.score(x_test_FULL, y_test_FULL).round(4)\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('ARD Train-Test Gap :', abs(ard_train_score - ard_test_score).round(4))\n",
    "ard_test_gap = abs(ard_train_score - ard_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "ard_model_values = zip(chef.columns, ard_fit.coef_.round(decimals = 5))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "ard_model_lst = [('intercept', ard_fit.intercept_.round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in ard_model_values:\n",
    "    ard_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in ard_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code may have to be run more than once ##\n",
    "\n",
    "# dropping coefficients that are equal to zero\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for feature, coefficient in ard_model_lst:\n",
    "        \n",
    "        if coefficient == 0:\n",
    "            ard_model_lst.remove((feature, coefficient))\n",
    "\n",
    "            \n",
    "# checking the results\n",
    "for pair in ard_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing explanatory variable data\n",
    "chef_ks_data = chef.drop(['REVENUE',\n",
    "                              'NAME',\n",
    "                              'EMAIL',\n",
    "                              'FIRST_NAME',\n",
    "                              'FAMILY_NAME',\n",
    "                              'EMAIL_DOMAIN',\n",
    "                              'email_domain_group'],\n",
    "                         axis = 1)\n",
    "\n",
    "# preparing the target variable\n",
    "chef_ks_target = chef.loc[ :, 'REVENUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing results\n",
    "\n",
    "print(f\"\"\"\n",
    "Model      Train Score      Test Score\n",
    "-----      -----------      ----------\n",
    "OLS        {lr_train_score}            {lr_test_score}\n",
    "Lasso      {lasso_train_score}            {lasso_test_score}\n",
    "ARD        {ard_train_score}            {ard_test_score}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Type'    : ['OLS', 'Lasso', 'ARD'],\n",
    "           \n",
    "    'Training' : [lr_train_score, lasso_train_score,\n",
    "                                   ard_train_score],\n",
    "           \n",
    "    'Testing'  : [lr_test_score, lasso_test_score,\n",
    "                                   ard_test_score],\n",
    "                    \n",
    "    'Train-Test Gap' : [lr_test_gap, lasso_test_gap,\n",
    "                                        ard_test_gap],\n",
    "                    \n",
    "    'Model Size' : [len(lr_model_lst), len(lasso_model_lst),\n",
    "                                    len(ard_model_lst)],\n",
    "                    \n",
    "    'Model' : [lr_model_lst, lasso_model_lst, ard_model_lst]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance_chef = pd.DataFrame(model_performance)\n",
    "\n",
    "\n",
    "# sending model results to Excel\n",
    "model_performance_chef.to_excel('./model_results/linear_model_performance_chef.xlsx',\n",
    "                           index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_results = pd.DataFrame(data = {\n",
    "    'Original Revenue'     : y_test_FULL,\n",
    "    'LR Predictions'       : lr_pred.round(decimals = 2),\n",
    "    'Lasso Predictions'    : lasso_pred.round(decimals = 2),\n",
    "    'ARD Predictions'      : ard_pred.round(decimals = 2),\n",
    "    'LR Deviation'         : lr_pred.round(decimals = 2) - y_test_FULL,\n",
    "    'Lasso Deviation'      : lasso_pred.round(decimals = 2) - y_test_FULL,\n",
    "    'ARD Deviation'        : ard_pred.round(decimals = 2) - y_test_FULL,\n",
    "    })\n",
    "\n",
    "\n",
    "prediction_results.to_excel(excel_writer = './model_results/linear_model_predictions_chef.xlsx',\n",
    "                            index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying file names\n",
    "\n",
    "performance_file = './model_results/linear_model_performance_chef.xlsx'\n",
    "prediction_file  = './model_results/linear_model_predictions_chef.xlsx'\n",
    "\n",
    "# reading files into Python\n",
    "performance = pd.read_excel(performance_file)\n",
    "predictions = pd.read_excel(prediction_file)\n",
    "\n",
    "\n",
    "performance.head(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing explanatory variable data\n",
    "chef_ks_data = chef.drop(['REVENUE',\n",
    "                              'NAME',\n",
    "                              'EMAIL',\n",
    "                              'FIRST_NAME',\n",
    "                              'FAMILY_NAME',\n",
    "                              'EMAIL_DOMAIN',\n",
    "                              'email_domain_group'],\n",
    "                         axis = 1)\n",
    "\n",
    "# preparing the target variable\n",
    "chef_ks_target = chef.loc[ :, 'REVENUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_ks_data,\n",
    "            chef_ks_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a KNN model object\n",
    "knn_reg = KNeighborsRegressor(algorithm = 'auto',\n",
    "                              n_neighbors = 1)\n",
    "\n",
    "\n",
    "# FITTING to the training data\n",
    "knn_fit = knn_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "knn_reg_pred = knn_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('KNN Training Score:', knn_reg.score(X_train, y_train).round(4))\n",
    "print('KNN Testing Score :',  knn_reg.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "knn_reg_score_train = knn_reg.score(X_train, y_train).round(4)\n",
    "knn_reg_score_test  = knn_reg.score(X_test, y_test).round(4)\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('KNN Train-Test Gap:', abs(knn_reg_score_train - knn_reg_score_test).round(4))\n",
    "knn_reg_test_gap = abs(knn_reg_score_train - knn_reg_score_test).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists for training set accuracy and test set accuracy\n",
    "training_accuracy = []\n",
    "test_accuracy     = []\n",
    "\n",
    "\n",
    "# building a visualization of 1 to 50 neighbors\n",
    "neighbors_settings = range(1, 21)\n",
    "\n",
    "\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # Building the model\n",
    "    clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Recording the training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "    # Recording the generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# plotting the visualization\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the optimal number of neighbors\n",
    "opt_neighbors = test_accuracy.index(max(test_accuracy)) + 1\n",
    "print(f\"\"\"The optimal number of neighbors is {opt_neighbors}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a model with the optimal number of neighbors\n",
    "knn_opt = KNeighborsRegressor(algorithm = 'auto',\n",
    "                              n_neighbors = opt_neighbors)\n",
    "\n",
    "\n",
    "\n",
    "# FITTING the model based on the training data\n",
    "knn_opt_fit = knn_opt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# PREDITCING on new data\n",
    "knn_opt_pred = knn_opt_fit.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('KNN Training Score:', knn_opt.score(X_train, y_train).round(4))\n",
    "print('KNN Testing Score :',  knn_opt.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "knn_opt_score_train = knn_opt.score(X_train, y_train).round(4)\n",
    "knn_opt_score_test  = knn_opt.score(X_test, y_test).round(4)\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('KNN Train-Test Gap:', abs(knn_reg_score_train - knn_reg_score_test).round(4))\n",
    "knn_opt_test_gap = abs(knn_reg_score_train - knn_reg_score_test).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the scaler with housing_data\n",
    "scaler.fit(chef_ks_data)\n",
    "\n",
    "\n",
    "# TRANSFORMING our data after fit\n",
    "X_scaled = scaler.transform(chef_ks_data)\n",
    "\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "X_scaled_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Unscaled Dataset\n",
    "##############################################################################\n",
    "\n",
    "# subsetting the original dataset\n",
    "chef_subset = chef_ks_data.loc[ : , ['log_TOTAL_MEALS_ORDERED',\n",
    "                                     'MEDIAN_MEAL_RATING',\n",
    "                                     'AVG_PREP_VID_TIME',\n",
    "                                     'MASTER_CLASSES_ATTENDED',\n",
    "                                     'LARGEST_ORDER_SIZE',\n",
    "                                     'has_UNIQUE_MEALS_PURCH',\n",
    "                                     'has_AVG_CLICKS_PER_VISIT']]\n",
    "\n",
    "\n",
    "\n",
    "# UNSCALED correlation matrix\n",
    "df_corr = chef_subset.corr().round(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the exact code we were using before\n",
    "X_train_STAND, X_test_STAND, y_train_STAND, y_test_STAND = train_test_split(\n",
    "            X_scaled_df,\n",
    "            chef_ks_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a model with the optimal number of neighbors\n",
    "knn_stand = KNeighborsRegressor(algorithm = 'auto',\n",
    "                                n_neighbors = 5)\n",
    "\n",
    "\n",
    "\n",
    "# FITTING the model based on the training data\n",
    "knn_stand_fit = knn_stand.fit(X_train_STAND, y_train_STAND)\n",
    "\n",
    "\n",
    "\n",
    "# PREDITCING on new data\n",
    "knn_stand_pred = knn_stand_fit.predict(X_test_STAND)\n",
    "\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('KNN Training Score:', knn_stand.score(X_train_STAND, y_train_STAND).round(4))\n",
    "print('KNN Testing Score :',  knn_stand.score(X_test_STAND, y_test_STAND).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "knn_stand_score_train = knn_stand.score(X_train_STAND, y_train_STAND).round(4)\n",
    "knn_stand_score_test  = knn_stand.score(X_test_STAND, y_test_STAND).round(4)\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('KNN Train-Test Gap:', abs(knn_stand_score_train - knn_stand_score_test).round(4))\n",
    "knn_stand_test_gap = abs(knn_stand_score_train - knn_stand_score_test).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing results\n",
    "\n",
    "print(f\"\"\"\n",
    "Model      Train Score      Test Score\n",
    "-----      -----------      ----------\n",
    "OLS        {lr_train_score}            {lr_test_score}\n",
    "Lasso      {lasso_train_score}            {lasso_test_score}\n",
    "ARD        {ard_train_score}            {ard_test_score}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Type'    : ['OLS', 'Lasso', 'ARD'],\n",
    "           \n",
    "    'Training' : [lr_train_score, lasso_train_score,\n",
    "                                   ard_train_score],\n",
    "           \n",
    "    'Testing'  : [lr_test_score, lasso_test_score,\n",
    "                                   ard_test_score],\n",
    "                    \n",
    "    'Train-Test Gap' : [lr_test_gap, lasso_test_gap,\n",
    "                                        ard_test_gap],\n",
    "                    \n",
    "    'Model Size' : [len(lr_model_lst), len(lasso_model_lst),\n",
    "                                    len(ard_model_lst)],\n",
    "                    \n",
    "    'Model' : [lr_model_lst, lasso_model_lst, ard_model_lst]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance_chef = pd.DataFrame(model_performance)\n",
    "\n",
    "\n",
    "# sending model results to Excel\n",
    "model_performance_chef.to_excel('./model_results/linear_model_performance_chef.xlsx',\n",
    "                           index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing results\n",
    "\n",
    "print(f\"\"\"\n",
    "KNN Model             Neighbors     Train Score      Test Score\n",
    "----------------      ---------     ----------       ----------\n",
    "Non-Standardized      1              {knn_reg_score_train}              {knn_reg_score_test}\n",
    "Non-Standardized      14             {knn_opt_score_train}           {knn_opt_score_test}\n",
    "Standardized          19             {knn_stand_score_train}           {knn_stand_score_test}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Type'    : ['KNN_Not_Standardized', 'KNN_Not_Standardized_Opt', 'KNN_Standardized_Opt'],\n",
    "           \n",
    "    \n",
    "    'Training' : [knn_reg_score_train,\n",
    "                  knn_opt_score_train,\n",
    "                  knn_stand_score_train],\n",
    "           \n",
    "    \n",
    "    'Testing'  : [knn_reg_score_test,\n",
    "                  knn_opt_score_test,\n",
    "                  knn_stand_score_test],\n",
    "                    \n",
    "    \n",
    "    'Train-Test Gap' : [knn_reg_test_gap,\n",
    "                        knn_opt_test_gap,\n",
    "                        knn_stand_test_gap],\n",
    "    \n",
    "    'Model Size' : ['NA', 'NA' , 'NA'],\n",
    "    \n",
    "    'Model'      : [\"NA\", \"NA\", \"NA\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting model_performance into a DataFrame\n",
    "knn_predictions = pd.DataFrame({\n",
    "    'KNN' : knn_stand_pred.round(decimals = 2),\n",
    "    'KNN Deviations' : knn_stand_pred.round(decimals = 2) - predictions['Original Revenue']})\n",
    "\n",
    "\n",
    "# concatenating with former performance DataFrame\n",
    "all_predictions = pd.concat([predictions, knn_predictions],\n",
    "                              axis = 1)\n",
    "\n",
    "\n",
    "# sending model results to Excel\n",
    "all_predictions.to_excel('./datasets/linear_model_performance_chef.xlsx',\n",
    "                           index = False)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "all_predictions.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "\n",
    "# concatenating with former performance DataFrame\n",
    "total_performance = pd.concat([performance, model_performance],\n",
    "                              axis = 0)\n",
    "\n",
    "\n",
    "total_performance.sort_values(by = 'Testing',\n",
    "                              ascending = False)\n",
    "\n",
    "\n",
    "# sending model results to Excel\n",
    "total_performance.to_excel('linear_model_performance_chef.xlsx',\n",
    "                           index = False)\n",
    "\n",
    "# checking the results\n",
    "total_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Cross Sell Success Models </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis variables v corss sell succes\n",
    "df_corr = chef.corr(method = 'pearson')\n",
    "\n",
    "df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring explanatory variables\n",
    "chef_data = chef.drop('CROSS_SELL_SUCCESS', axis = 1)\n",
    "\n",
    "\n",
    "# declaring response variable\n",
    "chef_target = chef.loc[ : , 'CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = chef_target) # Target is stratify to balance proportions o 0 and 1 for Cross_Sell_SUCCESS\n",
    "\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logistic_all = smf.logit(formula = \"\"\"CROSS_SELL_SUCCESS ~      REVENUE + \n",
    " TOTAL_MEALS_ORDERED + \n",
    " UNIQUE_MEALS_PURCH + \n",
    " CONTACTS_W_CUSTOMER_SERVICE + \n",
    " PRODUCT_CATEGORIES_VIEWED + \n",
    " AVG_TIME_PER_SITE_VISIT + \n",
    " MOBILE_NUMBER + \n",
    " CANCELLATIONS_BEFORE_NOON + \n",
    " CANCELLATIONS_AFTER_NOON + \n",
    " TASTES_AND_PREFERENCES + \n",
    " PC_LOGINS + \n",
    " MOBILE_LOGINS + \n",
    " WEEKLY_PLAN + \n",
    " EARLY_DELIVERIES + \n",
    " LATE_DELIVERIES + \n",
    " PACKAGE_LOCKER + \n",
    " REFRIGERATED_LOCKER + \n",
    " AVG_PREP_VID_TIME + \n",
    " LARGEST_ORDER_SIZE + \n",
    " MASTER_CLASSES_ATTENDED + \n",
    " MEDIAN_MEAL_RATING + \n",
    " AVG_CLICKS_PER_VISIT + \n",
    " TOTAL_PHOTOS_VIEWED + \n",
    " has_freq + \n",
    " has_contact + \n",
    " has_revenue + \n",
    " has_TOTAL_PHOTOS_VIEWED + \n",
    " has_MASTER_CLASSES_ATTENDED + \n",
    " has_EARLY_DELIVERIES + \n",
    " has_WEEKLY_PLAN + \n",
    " has_AVG_CLICKS_PER_VISIT + \n",
    " has_professional + \n",
    " has_personal + \n",
    " has_PRODUCT_CATEGORIES_VIEWED + \n",
    " has_MOBILE_LOGINS                                \n",
    "                                                             \"\"\",                                                \n",
    "                           data    = chef_train)\n",
    "\n",
    "     \n",
    "# fitting the model object\n",
    "logis_all = logistic_all.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logis_all.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing a model where all features are significn based on thier p-values\n",
    "\n",
    "# instantiating a logistic regression model object\n",
    "logit_ace = smf.logit(formula = \"\"\"CROSS_SELL_SUCCESS ~  \n",
    "                                                         has_revenue +\n",
    "                                                         has_professional +\n",
    "                                                         CANCELLATIONS_BEFORE_NOON  \n",
    "                                                          \"\"\",\n",
    "                          data = chef_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "logit_ace = logit_ace.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_ace.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory sets from last session\n",
    "\n",
    "# creating a dictionary to store candidate models\n",
    "\n",
    "\n",
    " # full model\n",
    "candidate_dict = {\n",
    "    'logit_full'   : [    'REVENUE', \n",
    " 'TOTAL_MEALS_ORDERED', \n",
    " 'UNIQUE_MEALS_PURCH', \n",
    " 'CONTACTS_W_CUSTOMER_SERVICE', \n",
    " 'PRODUCT_CATEGORIES_VIEWED', \n",
    " 'AVG_TIME_PER_SITE_VISIT', \n",
    " 'MOBILE_NUMBER', \n",
    " 'CANCELLATIONS_BEFORE_NOON', \n",
    " 'CANCELLATIONS_AFTER_NOON', \n",
    " 'TASTES_AND_PREFERENCES', \n",
    " 'PC_LOGINS', \n",
    " 'MOBILE_LOGINS', \n",
    " 'WEEKLY_PLAN', \n",
    " 'EARLY_DELIVERIES', \n",
    " 'LATE_DELIVERIES', \n",
    " 'PACKAGE_LOCKER', \n",
    " 'REFRIGERATED_LOCKER', \n",
    " 'AVG_PREP_VID_TIME', \n",
    " 'LARGEST_ORDER_SIZE', \n",
    " 'MASTER_CLASSES_ATTENDED', \n",
    " 'MEDIAN_MEAL_RATING', \n",
    " 'AVG_CLICKS_PER_VISIT', \n",
    " 'TOTAL_PHOTOS_VIEWED', \n",
    " 'has_freq', \n",
    " 'has_contact', \n",
    " 'has_TOTAL_PHOTOS_VIEWED', \n",
    " 'has_MASTER_CLASSES_ATTENDED', \n",
    " 'has_EARLY_DELIVERIES', \n",
    " 'has_WEEKLY_PLAN', \n",
    " 'has_AVG_CLICKS_PER_VISIT', \n",
    " 'has_professional', \n",
    " 'has_personal', \n",
    " 'has_PRODUCT_CATEGORIES_VIEWED', \n",
    " 'has_MOBILE_LOGINS'  ],\n",
    " \n",
    "\n",
    " # significant variables only (set 1)\n",
    "    'logit_sig'    : [   'MOBILE_NUMBER' ,  \n",
    "                         'AVG_PREP_VID_TIME' ,  \n",
    "                         'has_professional' ,\n",
    "                         'has_freq' ,\n",
    "                         'has_contact' ,\n",
    "                         'EARLY_DELIVERIES' ,\n",
    "                         'CANCELLATIONS_BEFORE_NOON' ,\n",
    "                         'WEEKLY_PLAN' ,\n",
    "                         'PC_LOGINS',\n",
    "                         'has_revenue',\n",
    "                         'has_personal'\n",
    "                     ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "chef_data   =  chef.loc[ : , candidate_dict['logit_sig']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LogReg Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\n",
    "print('LogReg Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(X_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a confusion matrix\n",
    "print(confusion_matrix(y_true = y_test,\n",
    "                       y_pred = logreg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area under the roc curve (auc)\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the models coefficients\n",
    "\n",
    "\n",
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(chef[candidate_dict['logit_sig']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in logreg_model_values:\n",
    "    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in logreg_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined function to display_tree\n",
    "\n",
    "def display_tree(tree, feature_df, height = 500, width = 800):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    tree       : fitted tree model object\n",
    "        fitted CART model to visualized\n",
    "    feature_df : DataFrame\n",
    "        DataFrame of explanatory features (used to generate labels)\n",
    "    height     : int, default 500\n",
    "        height in pixels to which to constrain image in html\n",
    "    width      : int, default 800\n",
    "        width in pixels to which to constrain image in html\n",
    "    \"\"\"\n",
    "\n",
    "    # visualizing the tree\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    \n",
    "    # exporting tree to graphviz\n",
    "    export_graphviz(decision_tree      = tree,\n",
    "                    out_file           = dot_data,\n",
    "                    filled             = True,\n",
    "                    rounded            = True,\n",
    "                    special_characters = True,\n",
    "                    feature_names      = feature_df.columns)\n",
    "\n",
    "\n",
    "    # declaring a graph object\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "\n",
    "    # creating image\n",
    "    img = Image(graph.create_png(),\n",
    "                height = height,\n",
    "                width  = width)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "# user defined function to plot_feature_importances\n",
    "\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier(max_depth = 4,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                     random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(X_train,\n",
    "                                                    y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(X_test,\n",
    "                                                    y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(X_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "full_tree_tn, \\\n",
    "full_tree_fp, \\\n",
    "full_tree_fn, \\\n",
    "full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {full_tree_tn}\n",
    "False Positives: {full_tree_fp}\n",
    "False Negatives: {full_tree_fn}\n",
    "True Positives : {full_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling display_tree\n",
    "display_tree(tree       = full_tree_fit,\n",
    "             feature_df = X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "pruned_tree = DecisionTreeClassifier(max_depth = 2,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                     random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "pruned_tree_fit  = pruned_tree.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "pruned_tree_pred = pruned_tree_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', pruned_tree_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', pruned_tree_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = pruned_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "pruned_tree_train_score = pruned_tree_fit.score(X_train, y_train).round(4) # accuracy\n",
    "pruned_tree_test_score  = pruned_tree_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving auc score\n",
    "pruned_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = pruned_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "pruned_tree_tn, \\\n",
    "pruned_tree_fp, \\\n",
    "pruned_tree_fn, \\\n",
    "pruned_tree_tp = confusion_matrix(y_true = y_test, y_pred = pruned_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling display_tree\n",
    "display_tree(tree       = pruned_tree_fit,\n",
    "             feature_df = X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting feature importance\n",
    "plot_feature_importances(pruned_tree_fit,\n",
    "                         train  = X_train,\n",
    "                         export = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score      TN, FP, FN, TP\n",
    "-----         ---------      --------------\n",
    "Logistic      {logreg_auc_score}         {logreg_tn, logreg_fp, logreg_fn, logreg_tp}\n",
    "Full Tree     {full_tree_auc_score}         {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}\n",
    "Pruned Tree   {pruned_tree_auc_score}         {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'    : ['Logistic', 'Full Tree', 'Pruned Tree'],\n",
    "           \n",
    "    'AUC Score' : [logreg_auc_score, full_tree_auc_score, pruned_tree_auc_score],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, full_tree_train_score,\n",
    "                           pruned_tree_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, full_tree_test_score,\n",
    "                           pruned_tree_test_score],\n",
    "\n",
    "    'Confusion Matrix'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                           (full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp),\n",
    "                           (pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp)]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# user defined function optimal_neighbors\n",
    "def optimal_neighbors(X_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.25,\n",
    "                      seed=219,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "X_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the X data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 219\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    if standardize == True:\n",
    "        # optionally standardizing X_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(X_data)\n",
    "        X_scaled           = scaler.transform(X_data)\n",
    "        X_scaled_df        = pd.DataFrame(X_scaled)\n",
    "        X_data             = X_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1\n",
    "\n",
    "\n",
    "# user defined function visual_cm\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the optimal number of neighbors\n",
    "opt_neighbors = optimal_neighbors(X_data        = chef_data,\n",
    "                                  y_data        = chef_target,\n",
    "                                  response_type = 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the data\n",
    "scaler.fit(chef_data)\n",
    "\n",
    "\n",
    "# TRANSFORMING the data\n",
    "X_scaled     = scaler.transform(chef_data)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_scaled_df  = pd.DataFrame(X_scaled) \n",
    "\n",
    "\n",
    "# train-test split with the scaled data\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "            X_scaled_df,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size = 0.25,\n",
    "            stratify = chef_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a KNN classification model with optimal neighbors\n",
    "knn_opt = KNeighborsClassifier(n_neighbors = opt_neighbors)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "knn_fit = knn_opt.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_pred = knn_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', knn_fit.score(X_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', knn_fit.score(X_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data\n",
    "knn_train_score = knn_fit.score(X_train_scaled, y_train_scaled).round(4)\n",
    "knn_test_score  = knn_fit.score(X_test_scaled, y_test_scaled).round(4)\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "knn_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "Chef_data   =  chef.loc[ : , candidate_dict['logit_sig']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a logistic regression model with default values\n",
    "lr_default = LogisticRegression(solver = 'lbfgs',\n",
    "                                C = 4.0,\n",
    "                                warm_start = False,\n",
    "                                random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING the training data\n",
    "lr_default_fit = lr_default.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_default_pred = lr_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', lr_default_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', lr_default_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# SCORING with AUC\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_default_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = lr_default_fit.score(X_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = lr_default_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = lr_default_pred).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV\n",
    "\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "C_space          = pd.np.arange(0.1, 5.0, 0.1)\n",
    "warm_start_space = [True, False]\n",
    "solver_space     = ['newton-cg', 'sag', 'lbfgs']\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'C'          : C_space,\n",
    "              'warm_start' : warm_start_space,\n",
    "              'solver'     : solver_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "lr_tuned = LogisticRegression(random_state = 219,\n",
    "                              max_iter     = 300)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "                                 param_distributions = param_grid, # parameters to tune\n",
    "                                 cv                  = 3,          # how many folds in cross-validation\n",
    "                                 n_iter              = 250,        # number of combinations of hyperparameters to try\n",
    "                                 random_state        = 219,        # starting point for random sequence\n",
    "                                 scoring = make_scorer(\n",
    "                                           roc_auc_score,\n",
    "                                           needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "lr_tuned_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the best estimator for the model\n",
    "lr_tuned_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "lr_tuned = lr_tuned_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_tuned_pred = lr_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# print('LR Tuned Training ACCURACY:', lr_tuned.score(x_train, y_train).round(4))\n",
    "print('LR Tuned Testing  ACCURACY:', lr_tuned.score(X_test, y_test).round(4))\n",
    "print('LR Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "lr_tuned_train_score = lr_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "lr_tuned_test_score  = lr_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "lr_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = lr_tuned_pred).round(4) # auc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "lr_tuned_tn, \\\n",
    "lr_tuned_fp, \\\n",
    "lr_tuned_fn, \\\n",
    "lr_tuned_tp = confusion_matrix(y_true = y_test, y_pred = lr_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {lr_tuned_tn}\n",
    "False Positives: {lr_tuned_fp}\n",
    "False Negatives: {lr_tuned_fn}\n",
    "True Positives : {lr_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "lr_train_acc = lr_tuned.score(X_train, y_train).round(4)\n",
    "lr_test_acc  = lr_tuned.score(X_test, y_test).round(4)\n",
    "lr_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = lr_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'        : 'Tuned LR',\n",
    "                           'Training Accuracy' : lr_train_acc,\n",
    "                           'Testing Accuracy'  : lr_test_acc,\n",
    "                           'AUC Score'         : lr_auc,\n",
    "                           'Confusion Matrix'  : (lr_tuned_tn,\n",
    "                                                  lr_tuned_fp,\n",
    "                                                  lr_tuned_fn,\n",
    "                                                  lr_tuned_tp)},\n",
    "                           ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertuning on Classification Trees\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "criterion_space = ['gini', 'entropy']\n",
    "splitter_space  = ['best', 'random']\n",
    "depth_space     = pd.np.arange(1, 25, 1)\n",
    "leaf_space      = pd.np.arange(1, 100, 1)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'criterion'        : criterion_space,\n",
    "              'splitter'         : splitter_space,\n",
    "              'max_depth'        : depth_space,\n",
    "              'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "tuned_tree = DecisionTreeClassifier(max_depth = 4,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                     random_state = 219)\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "                                   param_distributions   = param_grid,\n",
    "                                   cv                    = 3,\n",
    "                                   n_iter                = 400,\n",
    "                                   random_state          = 219,\n",
    "                                   scoring = make_scorer(roc_auc_score,\n",
    "                                             needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_tree_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = tuned_tree_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = tree_tuned_pred).round(4) # auc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_tree_tn, \\\n",
    "tuned_tree_fp, \\\n",
    "tuned_tree_fn, \\\n",
    "tuned_tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_tree_tn}\n",
    "False Positives: {tuned_tree_fp}\n",
    "False Negatives: {tuned_tree_fn}\n",
    "True Positives : {tuned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tree_train_acc = tree_tuned.score(X_train, y_train).round(4)\n",
    "tree_test_acc  = tree_tuned.score(X_test, y_test).round(4)\n",
    "tree_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = tree_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'        : 'Tuned Tree',\n",
    "                           'Training Accuracy' : tree_train_acc,\n",
    "                           'Testing Accuracy'  : tree_test_acc,\n",
    "                           'AUC Score'         : tree_auc,\n",
    "                           'Confusion Matrix'  : (tuned_tree_tn,\n",
    "                                                  tuned_tree_fp,\n",
    "                                                  tuned_tree_fn,\n",
    "                                                  tuned_tree_tp)},\n",
    "                           ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default = RandomForestClassifier(n_estimators     = 100,\n",
    "                                    criterion        = 'gini',\n",
    "                                    max_depth        = 4,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    bootstrap        = True,\n",
    "                                    warm_start       = False,\n",
    "                                    random_state     = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_default_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rf_default_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rf_default_fit_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "rf_tn, \\\n",
    "rf_fp, \\\n",
    "rf_fn, \\\n",
    "rf_tp = confusion_matrix(y_true = y_test, y_pred = rf_default_fit_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {rf_tn}\n",
    "False Positives: {rf_fp}\n",
    "False Negatives: {rf_fn}\n",
    "True Positives : {rf_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "rf_train_acc = rf_default_fit.score(X_train, y_train).round(4)\n",
    "rf_test_acc  = rf_default_fit.score(X_test, y_test).round(4)\n",
    "rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = rf_default_fit_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Random Forest (Full)',\n",
    "                           'Training Accuracy'  : rf_train_acc,\n",
    "                           'Testing Accuracy'   : rf_test_acc,\n",
    "                           'AUC Score'          : rf_auc,\n",
    "                           'Confusion Matrix'   : (rf_tn,\n",
    "                                                   rf_fp,\n",
    "                                                   rf_fn,\n",
    "                                                   rf_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune the model's hyperparameters using RandomizedSearchCV.\n",
    "\n",
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "estimator_space  = pd.np.arange(100, 200, 25)\n",
    "leaf_space       = pd.np.arange(1, 20, 5)\n",
    "criterion_space  = ['gini', 'entropy']\n",
    "bootstrap_space  = [True, False]\n",
    "warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'n_estimators'     : estimator_space,\n",
    "              'min_samples_leaf' : leaf_space,\n",
    "              'criterion'        : criterion_space,\n",
    "              'bootstrap'        : bootstrap_space,\n",
    "              'warm_start'       : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "forest_cv = RandomizedSearchCV(estimator           = forest_grid,\n",
    "                               param_distributions = param_grid,\n",
    "                               cv         = 3,\n",
    "                               n_iter     = 300,\n",
    "                               scoring    = make_scorer(roc_auc_score,\n",
    "                                            needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "forest_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", forest_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimators based on RandomizedSearchCV\n",
    "forest_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_default = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                              learning_rate = 0.1,\n",
    "                                              n_estimators  = 100,\n",
    "                                              criterion     = 'friedman_mse',\n",
    "                                              max_depth     = 3,\n",
    "                                              warm_start    = False,\n",
    "                                              random_state  = 219)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_gbm_default_fit = full_gbm_default.fit(chef_data,chef_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_default_pred = full_gbm_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', full_gbm_default_fit.score(X_train, y_train).round(4))\n",
    "print('Testing ACCURACY :', full_gbm_default_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = full_gbm_default_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "gbm_default_tn, \\\n",
    "gbm_default_fp, \\\n",
    "gbm_default_fn, \\\n",
    "gbm_default_tp = confusion_matrix(y_true = y_test, y_pred = full_gbm_default_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_default_tn}\n",
    "False Positives: {gbm_default_fp}\n",
    "False Negatives: {gbm_default_fn}\n",
    "True Positives : {gbm_default_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = full_gbm_default_fit.score(X_train, y_train).round(4)\n",
    "gbm_test_acc  = full_gbm_default_fit.score(X_test, y_test).round(4)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = full_gbm_default_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'       : 'GBM (Full)',\n",
    "                          'Training Accuracy' : gbm_train_acc,\n",
    "                          'Testing Accuracy'  : gbm_test_acc,\n",
    "                          'AUC Score'         : gbm_auc,\n",
    "                          'Confusion Matrix'  : (gbm_default_tn,\n",
    "                                                 gbm_default_fp,\n",
    "                                                 gbm_default_fn,\n",
    "                                                 gbm_default_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "learn_space        = pd.np.arange(100, 500, 100)\n",
    "estimator_space    = pd.np.arange(100, 200, 25)\n",
    "depth_space        = pd.np.arange(10, 200, 20)\n",
    "warm_start_space   = [True, False]\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'learning_rate' : learn_space,\n",
    "              'max_depth'     : depth_space,\n",
    "              'n_estimators'  : estimator_space,\n",
    "              'warm_start'     : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_grid = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "full_gbm_cv = RandomizedSearchCV(estimator     = full_gbm_grid,\n",
    "                           param_distributions = param_grid,\n",
    "                           cv                  = 3,\n",
    "                           n_iter              = 320,\n",
    "                           random_state        = 219,\n",
    "                           scoring             = make_scorer(roc_auc_score,\n",
    "                                                 needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "full_gbm_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
